{
  "permissions": {
    "allow": [
      "Bash(npcsh-bench:*)",
      "Bash(ollama list:*)",
      "Bash(docker:*)",
      "Bash(pip install:*)",
      "Bash(ollama pull:*)",
      "Bash(ollama show:*)",
      "Bash(harbor:*)",
      "Bash(python3:*)",
      "Bash(curl:*)",
      "Bash(open:*)",
      "Bash(npcsh:*)",
      "Bash(pip show:*)",
      "Bash(git -C /Users/caug/npcww/npc-core/npcsh log --oneline -5)",
      "Bash(ls:*)",
      "Bash(grep:*)",
      "Bash(python:*)",
      "Bash(pip index:*)",
      "Bash(done)",
      "Bash(kill:*)",
      "Bash(xargs:*)",
      "Bash(# Check the job log for RuntimeError details grep -A5 \"\"RuntimeError\\\\|Error\\\\|error\\\\|FAIL\\\\|chess-best-move\\\\|qwen\"\" \"\"/Users/caug/npcww/npc-core/npcsh/jobs/2026-01-31__21-58-37/job.log\"\")",
      "Bash(NPCSH_CHAT_MODEL=\"qwen3:8b\" NPCSH_CHAT_PROVIDER=\"ollama\" NPCSH_STREAM_OUTPUT=0 NPCSH_DEFAULT_MODE=agent python3 -c \"\nimport sys, os\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcsh''\\)\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcpy''\\)\nos.environ[''NPCSH_CHAT_MODEL''] = ''qwen3:8b''\nos.environ[''NPCSH_CHAT_PROVIDER''] = ''ollama''\nos.environ[''NPCSH_STREAM_OUTPUT''] = ''0''\n\nfrom npcsh._state import ShellState, process_pipeline_command\n\nstate = ShellState\\(\\)\nstate, result = process_pipeline_command\\(''list the files in /tmp'', None, state\\)\nprint\\(''---RESULT---''\\)\nif isinstance\\(result, dict\\):\n    print\\(''Output:'', str\\(result.get\\(''output'', ''''\\)\\)[:500]\\)\n    print\\(''Tool calls made:'', bool\\(result.get\\(''tool_calls''\\)\\)\\)\nelse:\n    print\\(''Output:'', str\\(result\\)[:500]\\)\n\")",
      "Bash(NPCSH_CHAT_MODEL=\"qwen3:8b\" NPCSH_CHAT_PROVIDER=\"ollama\" NPCSH_STREAM_OUTPUT=0 NPCSH_DEFAULT_MODE=agent python3 -c \"\nimport sys, os\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcsh''\\)\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcpy''\\)\nos.environ[''NPCSH_CHAT_MODEL''] = ''qwen3:8b''\nos.environ[''NPCSH_CHAT_PROVIDER''] = ''ollama''\nos.environ[''NPCSH_STREAM_OUTPUT''] = ''0''\n\nfrom npcpy.npc_compiler import Team\nfrom npcpy.npc_sysenv import get_system_message\nfrom npcpy.llm_funcs import get_llm_response\nfrom npcsh._state import ShellState, collect_llm_tools, model_supports_tool_calls\n\nstate = ShellState\\(\\)\ntc = model_supports_tool_calls\\(''qwen3:8b'', ''ollama''\\)\nprint\\(''tool_capable:'', tc\\)\n\ntools_for_llm, tool_exec_map = collect_llm_tools\\(state\\)\nprint\\(''tools:'', [t[''function''][''name''] for t in tools_for_llm]\\)\n\nresult = get_llm_response\\(\n    ''List the files in /tmp'',\n    model=''qwen3:8b'',\n    provider=''ollama'',\n    npc=state.npc,\n    team=state.team,\n    messages=[],\n    stream=False,\n    auto_process_tool_calls=True,\n    tools=tools_for_llm,\n    tool_map=tool_exec_map,\n\\)\nprint\\(''---RESULT---''\\)\nprint\\(''Response:'', str\\(result.get\\(''response'', ''''\\)\\)[:300]\\)\nprint\\(''Tool calls:'', result.get\\(''tool_calls''\\)\\)\nmsgs = result.get\\(''messages'', []\\)\nfor m in msgs:\n    role = m.get\\(''role''\\)\n    if role == ''tool'':\n        print\\(f''Tool result [{m.get\\(\"\"name\"\"\\)}]:'', str\\(m.get\\(''content'',''''\\)\\)[:200]\\)\n\")",
      "Bash(NPCSH_CHAT_MODEL=\"qwen3:8b\" NPCSH_CHAT_PROVIDER=\"ollama\" NPCSH_STREAM_OUTPUT=0 python3 -c \"\nimport sys, os\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcsh''\\)\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcpy''\\)\nos.environ[''NPCSH_CHAT_MODEL''] = ''qwen3:8b''\nos.environ[''NPCSH_CHAT_PROVIDER''] = ''ollama''\n\nfrom npcsh._state import ShellState, collect_llm_tools\n\nstate = ShellState\\(\\)\nprint\\(''NPC:'', state.npc\\)\nprint\\(''NPC name:'', getattr\\(state.npc, ''name'', None\\)\\)\nprint\\(''NPC jinxs_dict:'', list\\(getattr\\(state.npc, ''jinxs_dict'', {}\\).keys\\(\\)\\) if state.npc else None\\)\nprint\\(\\)\ntools, tmap = collect_llm_tools\\(state\\)\nprint\\(''Tools count:'', len\\(tools\\)\\)\n\")",
      "Bash(NPCSH_CHAT_MODEL=\"qwen3:8b\" NPCSH_CHAT_PROVIDER=\"ollama\" NPCSH_STREAM_OUTPUT=0 python3 -c \"\nimport sys, os\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcsh''\\)\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcpy''\\)\nos.environ[''NPCSH_CHAT_MODEL''] = ''qwen3:8b''\nos.environ[''NPCSH_CHAT_PROVIDER''] = ''ollama''\n\nfrom npcpy.npc_compiler import Team\nfrom npcpy.npc_sysenv import get_system_message\nfrom npcpy.llm_funcs import get_llm_response\n\nteam = Team\\(team_path=os.path.expanduser\\(''~/.npcsh/npc_team''\\)\\)\nnpc = team.npcs[''corca'']\n\n# Build tools from NPC jinxs \\(same as collect_llm_tools does\\)\ntools_for_llm = []\ntool_exec_map = {}\nfor jname, jinx in npc.jinxs_dict.items\\(\\):\n    tdef = npc.jinx_tool_catalog.get\\(jname\\)\n    if tdef:\n        tools_for_llm.append\\(tdef\\)\n        tool_exec_map[jname] = jinx.execute\nprint\\(''Tools:'', [t[''function''][''name''] for t in tools_for_llm]\\)\n\nresult = get_llm_response\\(\n    ''List the files in /tmp'',\n    model=''qwen3:8b'',\n    provider=''ollama'',\n    npc=npc,\n    team=team,\n    messages=[],\n    stream=False,\n    auto_process_tool_calls=True,\n    tools=tools_for_llm,\n    tool_map=tool_exec_map,\n\\)\nprint\\(''---RESULT---''\\)\nprint\\(''Response:'', str\\(result.get\\(''response'', ''''\\)\\)[:300]\\)\nprint\\(''Tool calls:'', bool\\(result.get\\(''tool_calls''\\)\\)\\)\nfor m in result.get\\(''messages'', []\\):\n    if m.get\\(''role''\\) == ''tool'':\n        print\\(f''Tool [{m.get\\(\"\"name\"\"\\)}]:'', str\\(m.get\\(''content'',''''\\)\\)[:200]\\)\n\")",
      "Bash(NPCSH_CHAT_MODEL=\"llama3.2\" NPCSH_CHAT_PROVIDER=\"ollama\" python3 -c \"\nimport sys, os\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcsh''\\)\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcpy''\\)\nos.environ[''NPCSH_CHAT_MODEL''] = ''llama3.2''\nos.environ[''NPCSH_CHAT_PROVIDER''] = ''ollama''\n\nfrom npcpy.npc_compiler import Team\nfrom npcpy.llm_funcs import get_llm_response\n\nteam = Team\\(team_path=os.path.expanduser\\(''~/.npcsh/npc_team''\\)\\)\nnpc = team.npcs[''corca'']\n\ntools_for_llm = []\ntool_exec_map = {}\nfor jname, jinx in npc.jinxs_dict.items\\(\\):\n    tdef = npc.jinx_tool_catalog.get\\(jname\\)\n    if tdef:\n        tools_for_llm.append\\(tdef\\)\n        tool_exec_map[jname] = jinx.execute\nprint\\(''Tools:'', [t[''function''][''name''] for t in tools_for_llm]\\)\n\nresult = get_llm_response\\(\n    ''List the files in /tmp'',\n    model=''llama3.2'',\n    provider=''ollama'',\n    npc=npc,\n    team=team,\n    messages=[],\n    stream=False,\n    auto_process_tool_calls=True,\n    tools=tools_for_llm,\n    tool_map=tool_exec_map,\n\\)\nprint\\(''---RESULT---''\\)\nprint\\(''Response:'', str\\(result.get\\(''response'', ''''\\)\\)[:300]\\)\nprint\\(''Tool calls:'', bool\\(result.get\\(''tool_calls''\\)\\)\\)\nfor m in result.get\\(''messages'', []\\):\n    if m.get\\(''role''\\) == ''tool'':\n        print\\(f''Tool [{m.get\\(\"\"name\"\"\\)}]:'', str\\(m.get\\(''content'',''''\\)\\)[:200]\\)\n\")",
      "Bash(NPCSH_CHAT_MODEL=\"gemma3:4b\" NPCSH_CHAT_PROVIDER=\"ollama\" python3 -c \"\nimport sys, os\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcsh''\\)\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcpy''\\)\nos.environ[''NPCSH_CHAT_MODEL''] = ''gemma3:4b''\nos.environ[''NPCSH_CHAT_PROVIDER''] = ''ollama''\n\nfrom npcpy.npc_compiler import Team\nfrom npcpy.llm_funcs import get_llm_response\n\nteam = Team\\(team_path=os.path.expanduser\\(''~/.npcsh/npc_team''\\)\\)\nnpc = team.npcs[''corca'']\n\ntools_for_llm = []\ntool_exec_map = {}\nfor jname, jinx in npc.jinxs_dict.items\\(\\):\n    tdef = npc.jinx_tool_catalog.get\\(jname\\)\n    if tdef:\n        tools_for_llm.append\\(tdef\\)\n        tool_exec_map[jname] = jinx.execute\nprint\\(''Tools:'', [t[''function''][''name''] for t in tools_for_llm]\\)\n\nresult = get_llm_response\\(\n    ''List the files in /tmp'',\n    model=''gemma3:4b'',\n    provider=''ollama'',\n    npc=npc,\n    team=team,\n    messages=[],\n    stream=False,\n    auto_process_tool_calls=True,\n    tools=tools_for_llm,\n    tool_map=tool_exec_map,\n\\)\nprint\\(''---RESULT---''\\)\nprint\\(''Response:'', str\\(result.get\\(''response'', ''''\\)\\)[:300]\\)\nprint\\(''Tool calls:'', bool\\(result.get\\(''tool_calls''\\)\\)\\)\nfor m in result.get\\(''messages'', []\\):\n    if m.get\\(''role''\\) == ''tool'':\n        print\\(f''Tool [{m.get\\(\"\"name\"\"\\)}]:'', str\\(m.get\\(''content'',''''\\)\\)[:200]\\)\n\")",
      "Bash(NPCSH_CHAT_MODEL=\"gemma3:4b\" NPCSH_CHAT_PROVIDER=\"ollama\" python3 -c \"\nimport sys, os\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcsh''\\)\nsys.path.insert\\(0, ''/Users/caug/npcww/npc-core/npcpy''\\)\nos.environ[''NPCSH_CHAT_MODEL''] = ''gemma3:4b''\nos.environ[''NPCSH_CHAT_PROVIDER''] = ''ollama''\n\nfrom npcpy.npc_compiler import Team\nfrom npcpy.llm_funcs import check_llm_command\n\nteam = Team\\(team_path=os.path.expanduser\\(''~/.npcsh/npc_team''\\)\\)\nnpc = team.npcs[''corca'']\n\n# Get jinxs from NPC\njinxs = dict\\(npc.jinxs_dict\\) if npc.jinxs_dict else {}\nprint\\(''Jinxs:'', list\\(jinxs.keys\\(\\)\\)\\)\n\nresult = check_llm_command\\(\n    ''List the files in /tmp'',\n    model=''gemma3:4b'',\n    provider=''ollama'',\n    npc=npc,\n    team=team,\n    messages=[],\n    jinxs=jinxs,\n    tool_capable=False,\n\\)\nprint\\(''---RESULT---''\\)\nprint\\(''Output:'', str\\(result.get\\(''output'', ''''\\)\\)[:500]\\)\nexecs = result.get\\(''jinx_executions'', []\\)\nprint\\(''Jinx executions:'', len\\(execs\\)\\)\nfor e in execs:\n    print\\(f''  {e.get\\(\"\"name\"\"\\)}\\({e.get\\(\"\"inputs\"\"\\)}\\) -> {str\\(e.get\\(\"\"output\"\",\"\"\"\"\\)\\)[:150]}''\\)\n\")",
      "Bash(NPCSH_CHAT_MODEL=\"llama3.2\" NPCSH_CHAT_PROVIDER=\"ollama\" NPCSH_STREAM_OUTPUT=0 NPCSH_DEFAULT_MODE=agent python3:*)",
      "Bash(perl -ne 'next if /^\\\\s*[⣾⣽⣻⢿⡿⣟⣯⣷]/; print;')",
      "Bash(for d in /Users/caug/npcww/npc-core/npcsh/jobs/2026-02-01__12-10-{09,39,40}/)",
      "Bash(twine upload:*)",
      "Bash(find:*)",
      "Bash(/Users/caug/.pyenv/versions/npc/bin/harbor run:*)",
      "Bash(! test -d /Users/caug/npcww/npc-core/npcsh/npcsh/npc_team/jinxs/incognide)",
      "Bash(# Check the open file descriptors of one of the benchmark processes to find output paths lsof -p 63358)",
      "Bash(for dir in /Users/caug/npcww/npc-core/npcsh/jobs/2026-02-01__16-03-*)",
      "Bash(# Check what the agent actually output - command-2 is the npc run ls -la /Users/caug/npcww/npc-core/npcsh/jobs/2026-02-01__16-03-49/sqlite-with-gcov__ubHZN2L/agent/command-2/ cat /Users/caug/npcww/npc-core/npcsh/jobs/2026-02-01__16-03-49/sqlite-with-gcov__ubHZN2L/agent/command-2/stdout.txt)",
      "Bash(fi)",
      "Bash(git checkout:*)",
      "Bash(NPCSH_CHAT_MODEL=phi4 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_DEBUG=1 npcsh -c \"write hello to /tmp/test.txt\")",
      "Bash(for d in jobs/2026-02-01__21-4*)",
      "Bash(xxd:*)",
      "Bash(NPCSH_CHAT_MODEL=phi4 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_DEBUG=1 npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=llama3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_DEBUG=1 NPCSH_STREAM_OUTPUT=0 npcsh:*)",
      "Bash(tee:*)",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_DEBUG=1 NPCSH_STREAM_OUTPUT=0 npcsh:*)",
      "Bash(NPCSH_MODEL=mistral-small3.2 NPCSH_PROVIDER=ollama python:*)",
      "Bash(NPCSH_MODEL=mistral-small3.2 NPCSH_PROVIDER=ollama npcsh:*)",
      "Bash(NPCSH_MODEL=mistral-small3.2 NPCSH_PROVIDER=ollama NPCSH_LOG_LEVEL=DEBUG npcsh:*)",
      "Bash(for f in corca guac alicanto kadiefa frederic plonk)",
      "Bash(do diff \"/Users/caug/npcww/npc-core/npcsh/npcsh/npc_team/$f.npc\" \"/Users/caug/.npcsh/npc_team/$f.npc\")",
      "Bash(echo:*)",
      "Bash(for f in sh delegate)",
      "Bash(do diff:*)",
      "Bash(cat:*)",
      "WebSearch",
      "WebFetch(domain:github.com)",
      "WebFetch(domain:arxiv.org)",
      "WebFetch(domain:raw.githubusercontent.com)",
      "Bash(pkill:*)",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_OLLAMA_NUM_CTX=16384 npcsh -c \"Delegate to a specialist to write a Python script at /tmp/primes.py that finds all prime numbers under 100 and prints them\")",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_OLLAMA_NUM_CTX=16384 NPCSH_DEFAULT_MODE=agent npcsh -c \"Count how many lines in /etc/hosts contain a comment \\(lines starting with #\\) and write just the number to /tmp/comment_count.txt\")",
      "Bash(NPCSH_LOG_LEVEL=verbose NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_OLLAMA_NUM_CTX=16384 npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_OLLAMA_NUM_CTX=16384 npcsh -c \"Count how many lines in /etc/hosts contain a comment \\(lines starting with #\\) and write just the number to /tmp/comment_count.txt\")",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_OLLAMA_NUM_CTX=16384 PYTHONUNBUFFERED=1 python -c \"\nimport logging, sys\nlogging.basicConfig\\(level=logging.DEBUG, stream=sys.stderr, format=''%\\(name\\)s %\\(message\\)s''\\)\n\nfrom npcsh._state import ShellState, process_pipeline_command\nstate = ShellState\\(\\)\nstate, result = process_pipeline_command\\(\n    ''Count how many lines in /etc/hosts contain a comment \\(lines starting with #\\) and write just the number to /tmp/comment_count.txt'',\n    None,\n    state,\n    False\n\\)\nprint\\(''RESULT:'', result, file=sys.stderr\\)\nimport os\nprint\\(''FILE:'', open\\(''/tmp/comment_count.txt''\\).read\\(\\) if os.path.exists\\(''/tmp/comment_count.txt''\\) else ''NOT CREATED'', file=sys.stderr\\)\n\")",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_OLLAMA_NUM_CTX=16384 PYTHONUNBUFFERED=1 python -c \"\nimport logging, sys\nlogging.basicConfig\\(level=logging.DEBUG, stream=sys.stderr, format=''%\\(name\\)s %\\(message\\)s''\\)\n\nfrom npcsh._state import ShellState, process_pipeline_command\nstate = ShellState\\(\\)\nstate, result = process_pipeline_command\\(\n    ''Count how many lines in /etc/hosts contain a comment \\(lines starting with #\\) and write just the number to /tmp/comment_count.txt'',\n    None,\n    state,\n    False\n\\)\n\")",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_OLLAMA_NUM_CTX=16384 npcsh -c \"Search the web for who created Linux and save the answer to /tmp/linux_creator.txt\")",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_OLLAMA_NUM_CTX=16384 npcsh:*)",
      "Bash(bash -c 'test -f /tmp/comment_count.txt && grep -qE \"\"[0-9]+\"\" /tmp/comment_count.txt && echo PASS || echo FAIL')",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 timeout 120 npcsh -c \"Create a file /tmp/config.ini with the following sections: [database] with host=localhost and port=5432, and [app] with debug=true and name=myapp\")",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 timeout 120 npcsh -c \"Write a Python script /tmp/fib.py that defines a function fibonacci\\(n\\) returning the nth fibonacci number \\(0-indexed, fib\\(0\\)=0, fib\\(1\\)=1\\). The script should print fibonacci\\(10\\) when run.\")",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 timeout 120 npcsh:*)",
      "Bash(bash -c \"grep -q ''\\\\[database\\\\]'' /tmp/config.ini && grep -q ''host=localhost'' /tmp/config.ini && grep -q ''port=5432'' /tmp/config.ini && grep -q ''\\\\[app\\\\]'' /tmp/config.ini && grep -q ''debug=true'' /tmp/config.ini && echo PASS || echo FAIL\")",
      "Bash(test:*)",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_LOG_LEVEL=DEBUG timeout 180 npcsh -c \"Search the web for ''python asyncio tutorial'' and write a summary of what you found to /tmp/search_results.txt\")",
      "Bash(NPCSH_DEBUG=1 NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 timeout 180 npcsh -c \"Search the web for ''python asyncio tutorial'' and write a summary of what you found to /tmp/search_results.txt\")",
      "Bash(NPCSH_DEBUG=1 NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 timeout 60 npcsh -c \"Create a file /tmp/hello.txt containing exactly the text ''hello world''\")",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 timeout 60 npcsh -c \"Create a file /tmp/hello.txt containing exactly the text ''hello world''\")",
      "Bash(NPCSH_DEBUG=1 NPCSH_OLLAMA_NUM_CTX=16384 NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 timeout 180 npcsh -c \"Search the web for ''python asyncio tutorial'' and write a summary of what you found to /tmp/search_results.txt\")",
      "Bash(NPCSH_DEBUG=1 NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 timeout 300 npcsh -c \"Search the web for ''python asyncio tutorial'' and write a summary of what you found to /tmp/search_results.txt\")",
      "Bash(pdflatex:*)",
      "WebFetch(domain:opencode.ai)",
      "WebFetch(domain:docs.ollama.com)",
      "WebFetch(domain:pypi.org)",
      "WebFetch(domain:ollama.com)",
      "WebFetch(domain:deepwiki.com)",
      "Bash(bash:*)",
      "Bash(npm install:*)",
      "Bash(claude --version:*)",
      "Bash(opencode --version:*)",
      "Bash(nanocoder --version:*)",
      "Bash(export PATH=\"$HOME/.opencode/bin:$PATH\")",
      "Bash(opencode:*)",
      "Bash(ANTHROPIC_AUTH_TOKEN=ollama ANTHROPIC_BASE_URL=http://localhost:11434 claude -p \"Create a file /tmp/hello.txt containing exactly the text ''hello world'' \\(no quotes\\)\" --dangerously-skip-permissions --model mistral-small3.2)",
      "Bash(nanocoder run:*)",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_OLLAMA_NUM_CTX=32768 npcsh:*)",
      "Bash(~/.opencode/bin/opencode:*)",
      "Bash(nanocoder:*)",
      "Bash(ANTHROPIC_AUTH_TOKEN=ollama ANTHROPIC_BASE_URL=http://localhost:11434 timeout 120 claude -p \"Create a file /tmp/hello5.txt containing exactly the text ''hello world'' \\(no quotes\\). Use the bash tool to run: echo ''hello world'' > /tmp/hello5.txt\" --dangerously-skip-permissions --model mistral-small3.2)",
      "Bash(CLAUDECODE=\"\" CLAUDE_CODE_ENTRYPOINT=\"\" ANTHROPIC_AUTH_TOKEN=ollama ANTHROPIC_BASE_URL=http://localhost:11434 DISABLE_AUTOUPDATER=1 claude -p \"Create a file /tmp/hello_cc.txt containing exactly the text hello world\" --dangerously-skip-permissions --model mistral-small3.2 --verbose)",
      "Bash(CLAUDECODE=\"\" CLAUDE_CODE_ENTRYPOINT=\"\" ANTHROPIC_AUTH_TOKEN=ollama ANTHROPIC_BASE_URL=http://localhost:11434 DISABLE_AUTOUPDATER=1 CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 timeout 120 claude -p \"Create a file /tmp/hello_cc.txt containing exactly the text hello world\" --dangerously-skip-permissions --model mistral-small3.2)",
      "Bash(ollama:*)",
      "Bash(CLAUDECODE=\"\" CLAUDE_CODE_ENTRYPOINT=\"\" ANTHROPIC_AUTH_TOKEN=ollama ANTHROPIC_BASE_URL=http://localhost:11434 DISABLE_AUTOUPDATER=1 CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 timeout 90 claude -p \"Create a file /tmp/hello_cc.txt containing exactly the text hello world\" --dangerously-skip-permissions --model qwen3-coder)",
      "Bash(CLAUDECODE=\"\" CLAUDE_CODE_ENTRYPOINT=\"\" ANTHROPIC_AUTH_TOKEN=ollama ANTHROPIC_BASE_URL=http://localhost:11434 DISABLE_AUTOUPDATER=1 CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 timeout 120 claude -p \"Create a file /tmp/hello_cc2.txt containing hello world\" --dangerously-skip-permissions --model mistral-small3.2)",
      "Bash(ANTHROPIC_AUTH_TOKEN=ollama ANTHROPIC_BASE_URL=http://localhost:11434 DISABLE_AUTOUPDATER=1 CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 timeout 120 env -u CLAUDECODE -u CLAUDE_CODE_ENTRYPOINT claude -p \"Create a file /tmp/test_cc_mistral.txt containing exactly the text ''hello from mistral''\" --dangerously-skip-permissions --model mistral-small3.2)",
      "Bash(ANTHROPIC_AUTH_TOKEN=ollama ANTHROPIC_BASE_URL=http://localhost:11434 DISABLE_AUTOUPDATER=1 CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 timeout 120 env -u CLAUDECODE -u CLAUDE_CODE_ENTRYPOINT claude -p \"Create a file /tmp/test_cc_phi4.txt containing exactly the text ''hello from phi4''\" --dangerously-skip-permissions --model phi4)",
      "Bash(ANTHROPIC_AUTH_TOKEN=ollama ANTHROPIC_BASE_URL=http://localhost:11434 DISABLE_AUTOUPDATER=1 CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 timeout 120 env -u CLAUDECODE -u CLAUDE_CODE_ENTRYPOINT claude -p \"Create a file /tmp/test_cc_llama.txt containing exactly the text ''hello from llama''\" --dangerously-skip-permissions --model llama3.2)",
      "Bash(ANTHROPIC_AUTH_TOKEN=ollama ANTHROPIC_BASE_URL=http://localhost:11434 DISABLE_AUTOUPDATER=1 CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 timeout 120 env -u CLAUDECODE -u CLAUDE_CODE_ENTRYPOINT claude -p \"Create a file /tmp/test_cc_gptoss.txt containing exactly the text ''hello from gptoss''\" --dangerously-skip-permissions --model gpt-oss:20b)",
      "Bash(ANTHROPIC_AUTH_TOKEN=ollama ANTHROPIC_BASE_URL=http://localhost:11434 DISABLE_AUTOUPDATER=1 CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 timeout 120 env -u CLAUDECODE -u CLAUDE_CODE_ENTRYPOINT claude -p \"Create a file /tmp/test_cc_qwen3.txt containing exactly the text ''hello from qwen3''\" --dangerously-skip-permissions --model qwen3:latest)",
      "Bash(NPCSH_CHAT_MODEL=llama3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 OLLAMA_HOST=http://localhost:11434 NPCSH_OLLAMA_NUM_CTX=32768 timeout 120 npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=gpt-oss:20b NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 OLLAMA_HOST=http://localhost:11434 NPCSH_OLLAMA_NUM_CTX=32768 timeout 120 npcsh -c \"Create a file /tmp/test_npc_gptoss.txt containing exactly the text ''hello from gptoss''\")",
      "Bash(NPCSH_CHAT_MODEL=qwen3:latest NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 OLLAMA_HOST=http://localhost:11434 NPCSH_OLLAMA_NUM_CTX=32768 timeout 120 npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=phi4 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 OLLAMA_HOST=http://localhost:11434 NPCSH_OLLAMA_NUM_CTX=32768 timeout 120 npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=gemma3:12b NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 OLLAMA_HOST=http://localhost:11434 NPCSH_OLLAMA_NUM_CTX=32768 timeout 120 npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=mistral-small3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 OLLAMA_HOST=http://localhost:11434 NPCSH_OLLAMA_NUM_CTX=32768 timeout 120 npcsh -c \"Create a file /tmp/test_npc_mistral.txt containing exactly the text ''hello from mistral''\")",
      "Bash(for f in /tmp/test_npc_*.txt /tmp/test_oc_*.txt /tmp/test_cc_*.txt)",
      "Bash(do if [ -f \"$f\" ])",
      "Bash(for expected in llama gptoss qwen3 phi4 gemma12b mistral)",
      "Bash(do for prefix in npc oc cc)",
      "Bash(do f=\"/tmp/test_$prefix_$expected.txt\")",
      "Bash([ ! -f \"$f\" ])",
      "Bash(for f in /tmp/test_npc_*.txt)",
      "Bash(/tmp/smoke_test_results.csv << 'EOF'\nframework,model,params,result,action_type,detail\nnpcsh,mistral-small3.2,24B,PASS,executed,\"sh echo > file\"\nnpcsh,qwen3:latest,8B,PASS,executed,\"sh echo > file\"\nnpcsh,gemma3:12b,12B,PASS,executed,\"file created \\(ReAct fallback - model has no native tool support\\)\"\nnpcsh,phi4,14B,PASS,executed,\"file created \\(ReAct fallback - model has no native tool support\\)\"\nnpcsh,gpt-oss:20b,20B,FAIL,empty_output,\"processed 58s then returned empty output\"\nnpcsh,llama3.2,3B,FAIL,attempted,\"emitted python tool call JSON in output but file not created\"\nopencode,mistral-small3.2,24B,FAIL,gave_advice,\"chatty response about how to create file\"\nopencode,qwen3:latest,8B,FAIL,gave_advice,\"asked for clarification instead of acting\"\nopencode,gemma3:12b,12B,FAIL,crashed,\"Error: does not support tools\"\nopencode,phi4,14B,FAIL,crashed,\"Error: does not support tools\"\nopencode,gpt-oss:20b,20B,FAIL,invalid_tool,\"Invalid Tool error then gave advice\"\nopencode,llama3.2,3B,FAIL,gave_advice,\"chatty todo list response\"\nclaude,mistral-small3.2,24B,FAIL,gave_advice,\"I don't have the capability to create files\"\nclaude,qwen3:latest,8B,FAIL,empty_output,\"exit code 0 but no output\"\nclaude,gemma3:12b,12B,FAIL,crashed,\"Error: does not support tools\"\nclaude,phi4,14B,FAIL,crashed,\"Error: does not support tools\"\nclaude,gpt-oss:20b,20B,FAIL,gave_advice,\"I'm ready to help - ignored instruction\"\nclaude,llama3.2,3B,FAIL,hallucinated,\"hallucinated a skill invocation that doesn't exist\"\nnanocoder,all_models,all,FAIL,crashed,\"Raw mode not supported - React/Ink requires TTY\"\nEOF)",
      "Bash(timeout 60 script -q /dev/null nanocoder run:*)",
      "Bash(timeout 120 expect:*)",
      "Bash(while read hash msg)",
      "Bash(do if git show \"$hash\":npcpy/llm_funcs.py)",
      "Bash(then echo \"$hash $msg\")",
      "Bash(for commit in 92ad70a 09bd512 8f96f2d 9261ef0 5400ce4 018006f ddf3860 7740668 1dfc0fb ddc6cae 5a6bd1e f348a2f 8ff13f2 dd0de7f 033cd94 e103590 0efa5e7 a8a40c9 a3cad61 f43d4c7 847793d 984ecca a3679d6 ec82516 2226a88 fa88012 2581d08 b91fdf3 469afd7)",
      "Bash(do echo \"COMMIT $commit:\")",
      "Bash(for pair in \"7740668..1dfc0fb\" \"1dfc0fb..ddc6cae\" \"ddc6cae..5a6bd1e\" \"5a6bd1e..f348a2f\" \"f348a2f..8ff13f2\" \"8ff13f2..dd0de7f\")",
      "Bash(do echo \"=== DIFF $pair ===\")",
      "Bash(for pair in \"dd0de7f..033cd94\" \"033cd94..e103590\" \"e103590..0efa5e7\" \"0efa5e7..a8a40c9\" \"a8a40c9..a3cad61\" \"a3cad61..f43d4c7\" \"f43d4c7..847793d\" \"847793d..984ecca\" \"984ecca..a3679d6\" \"a3679d6..ec82516\" \"ec82516..2226a88\" \"2226a88..fa88012\" \"fa88012..2581d08\" \"2581d08..b91fdf3\" \"b91fdf3..469afd7\")",
      "Bash(for pair in \"a3cad61..f43d4c7\" \"f43d4c7..847793d\" \"847793d..984ecca\" \"984ecca..a3679d6\" \"a3679d6..ec82516\" \"ec82516..2226a88\" \"2226a88..fa88012\" \"fa88012..2581d08\" \"2581d08..b91fdf3\" \"b91fdf3..469afd7\")",
      "Bash(git -C /Users/caug/npcww/npc-core/npcsh rev-parse --show-toplevel)",
      "Bash(git -C /Users/caug/npcww/npc-core/npcpy rev-parse:*)",
      "Bash(git -C /Users/caug/npcww/npc-core/npcpy log --oneline --all -- npcpy/npc_sysenv.py)",
      "Bash(git -C /Users/caug/npcww/npc-core/npcpy log -p --all -S \"tool_capable\" -- npcpy/npc_sysenv.py)",
      "Bash(git -C /Users/caug/npcww/npc-core/npcpy log -p --all -S \"jinx\" -- npcpy/npc_sysenv.py)",
      "Bash(git -C /Users/caug/npcww/npc-core/npcpy log -p --all -S \"handle_jinx\" -- npcpy/npc_sysenv.py)",
      "Bash(git -C /Users/caug/npcww/npc-core/npcpy show 7740668:npcpy/npc_sysenv.py)",
      "Bash(git -C /Users/caug/npcww/npc-core/npcpy show a8a40c9:npcpy/npc_sysenv.py)",
      "Bash(git -C /Users/caug/npcww/npc-core/npcpy show a3cad61:npcpy/npc_sysenv.py)",
      "Bash(chmod:*)",
      "Bash(NPCSH_CHAT_MODEL=gemma3:12b NPCSH_CHAT_PROVIDER=ollama npcsh:*)",
      "Bash(wc:*)",
      "Bash(gh pr list:*)",
      "Bash(gh pr view:*)",
      "Bash(gh pr diff:*)",
      "Bash(gh repo clone:*)",
      "Bash(gh pr checkout:*)",
      "Bash(lsof:*)",
      "Bash(# Kill all benchmark-related processes kill -9 22339 22342 22997 ; sleep 1 # Kill any remaining npcsh processes ps aux)",
      "Bash(while true)",
      "Bash(do)",
      "Bash(csv_file=\"$HOME/.npcsh/benchmarks/local/ollama_qwen3:4b_running.csv\")",
      "Bash(git -C /Users/caug/npcww/npc-core/npcsh log --all --format=\"%h %ai %s\" -- npcsh/npc_team/jinxs/modes/git.jinx)",
      "WebFetch(domain:docs.snowflake.com)",
      "WebFetch(domain:docs.databricks.com)",
      "WebFetch(domain:docs.cloud.google.com)",
      "Bash(npc nql_report:*)",
      "Bash(/Users/caug/.pyenv/versions/npc/bin/npc nql_snapshot:*)",
      "Bash(/Users/caug/.pyenv/versions/npc/bin/npc sql:*)",
      "Bash(/Users/caug/.pyenv/versions/npc/bin/python3:*)",
      "Bash(launchctl list:*)",
      "Bash(launchctl unload:*)",
      "Bash(/Users/caug/.pyenv/versions/npc/bin/pip install -e . --quiet)",
      "Bash(/Users/caug/.pyenv/versions/npc/bin/npc nql_report:*)",
      "Bash(/Users/caug/.pyenv/versions/npc/bin/python:*)",
      "Bash(/Users/caug/.pyenv/versions/npc/bin/npc web_search:*)",
      "Bash(/Users/caug/.pyenv/versions/npc/bin/npc:*)",
      "Bash(sqlite3:*)",
      "Bash(NPCSH_STREAM_OUTPUT=0 npcsh:*)",
      "Bash(NPCSH_STREAM_OUTPUT=0 NPCSH_CHAT_MODEL=qwen3:4b NPCSH_CHAT_PROVIDER=ollama npcsh:*)",
      "Bash(git stash:*)",
      "Bash(NPCSH_STREAM_OUTPUT=0 NPCSH_NO_EMBEDDINGS=1 NPCSH_CHAT_MODEL=gemma3:1b NPCSH_CHAT_PROVIDER=ollama NPCSH_OLLAMA_NUM_CTX=16384 timeout 60 npcsh -c \"write hello to /tmp/test_tool.txt\")",
      "Bash(NPCSH_STREAM_OUTPUT=0 NPCSH_NO_EMBEDDINGS=1 NPCSH_CHAT_MODEL=gemma3:1b NPCSH_CHAT_PROVIDER=ollama NPCSH_OLLAMA_NUM_CTX=16384 NPCSH_LOG_LEVEL=DEBUG timeout 60 npcsh -c \"write hello to /tmp/test_tool.txt\")",
      "Bash(NPCSH_CHAT_MODEL=ollama/gemma3:1b NPCSH_CHAT_PROVIDER=ollama NPCSH_LOG_LEVEL=DEBUG npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=gemma3:1b NPCSH_CHAT_PROVIDER=ollama npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=gemma3:4b NPCSH_CHAT_PROVIDER=ollama npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=gemma3:1b NPCSH_CHAT_PROVIDER=ollama timeout 60 npcsh:*)",
      "Bash(timeout 90 NPCSH_CHAT_MODEL=gemma3:1b NPCSH_CHAT_PROVIDER=ollama NPCSH_REASONING_MODEL=gemma3:1b NPCSH_REASONING_PROVIDER=ollama python3:*)",
      "Bash(NPCSH_CHAT_MODEL=gemma3:1b NPCSH_CHAT_PROVIDER=ollama timeout 90 python3:*)",
      "Bash(git rm:*)",
      "Bash(git cherry-pick:*)",
      "Bash(git add:*)",
      "Bash(NPCSH_CHAT_MODEL=gemma3:4b NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_NO_EMBEDDINGS=1 npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=gemma3:4b NPCSH_CHAT_PROVIDER=ollama NPCSH_STREAM_OUTPUT=0 NPCSH_NO_EMBEDDINGS=1 NPCSH_DIR_LISTING=0 npcsh -c \"search the web for the latest news\")",
      "Bash(NPCSH_DIR_LISTING=0 npcsh -c \"search the web for latest news about AI\")",
      "Bash(NPCSH_DIR_LISTING=0 python3:*)",
      "Bash(NPCSH_DIR_LISTING=0 python:*)",
      "Bash(NPCSH_DIR_LISTING=0 NPCSH_LOG_LEVEL=DEBUG npcsh:*)",
      "Bash(NPCSH_DIR_LISTING=0 NPCSH_MODEL=llama3.2 NPCSH_PROVIDER=ollama npcsh -c \"Search the web for who created Linux and save the answer to /tmp/linux_creator.txt\")",
      "Bash(NPCSH_DIR_LISTING=0 npcsh -c \"Search the web for today''s top news headlines and write a summary to /tmp/news_today.txt\")",
      "Bash(NPCSH_CHAT_MODEL=llama3.2 NPCSH_CHAT_PROVIDER=ollama timeout 60 npcsh -c \"list the files in /tmp and save the listing to /tmp/test_listing.txt\")",
      "Bash(NPCSH_CHAT_MODEL=gemma3:4b NPCSH_CHAT_PROVIDER=ollama timeout 60 npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=llama3.2 NPCSH_CHAT_PROVIDER=ollama timeout 60 npcsh -c \"create a file /tmp/llama_test.txt with the text ''hello from llama''\")",
      "Bash(NPCSH_CHAT_MODEL=llama3.2 NPCSH_CHAT_PROVIDER=ollama timeout 60 npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=llama3.2 NPCSH_CHAT_PROVIDER=ollama timeout 20 npcsh -c \"create a file /tmp/ll_debug.txt containing ''debug test''\")",
      "Bash(NPCSH_CHAT_MODEL=llama3.2 NPCSH_CHAT_PROVIDER=ollama timeout 20 npcsh -c \"create a file /tmp/ll_detail.txt containing ''detail test''\")",
      "Bash(NPCSH_CHAT_MODEL=llama3.2 NPCSH_CHAT_PROVIDER=ollama timeout 20 npcsh -c \"create a file /tmp/ll_detail2.txt containing ''detail test''\")",
      "Bash(NPCSH_CHAT_MODEL=gemma3:4b NPCSH_CHAT_PROVIDER=ollama timeout 45 npcsh -c \"search the web for latest AI news and save the results to /tmp/ai_news_test.txt\")",
      "Bash(NPCSH_CHAT_MODEL=llama3.2 NPCSH_CHAT_PROVIDER=ollama timeout 45 npcsh -c \"search the web for latest AI news and save the results to /tmp/ai_news_ll.txt\")",
      "Bash(NPCSH_CHAT_MODEL=gemma3:4b NPCSH_CHAT_PROVIDER=ollama timeout 45 npcsh -c \"search the web for latest AI news and save the results to /tmp/ai_news_g4b.txt\")",
      "Bash(NPCSH_CHAT_MODEL=gemma3:4b NPCSH_CHAT_PROVIDER=ollama timeout 45 npcsh -c \"search the web for latest AI news and save the results to /tmp/ai_news_g4b2.txt\")",
      "Bash(NPCSH_CHAT_MODEL=gemma3:4b NPCSH_CHAT_PROVIDER=ollama timeout 45 npcsh -c \"search the web for latest AI news and save the results to /tmp/ai_news_fix.txt\")",
      "Bash(NPCSH_CHAT_MODEL=llama3.2 NPCSH_CHAT_PROVIDER=ollama npcsh:*)",
      "Bash(NPCSH_MODEL=llama3.2 NPCSH_PROVIDER=ollama NPCSH_SEARCH_PROVIDER=perplexity npcsh:*)",
      "Bash(NPCSH_MODEL=gemma3:1b NPCSH_PROVIDER=ollama NPCSH_SEARCH_PROVIDER=perplexity npcsh:*)",
      "Bash(env:*)",
      "Bash(NPCSH_CHAT_MODEL=llama3.2 NPCSH_CHAT_PROVIDER=ollama NPCSH_SEARCH_PROVIDER=perplexity npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=gemma3:1b NPCSH_CHAT_PROVIDER=ollama NPCSH_SEARCH_PROVIDER=perplexity npcsh:*)",
      "Bash(NPCSH_CHAT_MODEL=gemma3:4b NPCSH_CHAT_PROVIDER=ollama NPCSH_SEARCH_PROVIDER=perplexity npcsh:*)",
      "Bash(for model in \"gemma3:4b\" \"gemma3:1b\" \"llama3.2\")",
      "Bash(f=\"$HOME/.npcsh/benchmarks/local/ollama_$model_running.csv\")",
      "Bash(if [ -f \"$f\" ])",
      "Bash(then)",
      "Bash(else)"
    ]
  }
}
