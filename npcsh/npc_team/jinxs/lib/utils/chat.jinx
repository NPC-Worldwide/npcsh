jinx_name: chat
description: Simple chat mode - LLM conversation without tool execution
inputs:
- query: null
- model: null
- provider: null
- stream: true

steps:
  - name: chat_response
    engine: python
    code: |
      from npcpy.llm_funcs import get_llm_response
      from npcpy.npc_sysenv import print_and_process_stream_with_markdown

      npc = context.get('npc')
      messages = context.get('messages', [])
      query = context.get('query', '')
      stream = context.get('stream', True)

      model = context.get('model') or (npc.model if npc else None)
      provider = context.get('provider') or (npc.provider if npc else None)

      if not query:
          context['output'] = ''
          context['messages'] = messages
      else:
          response = get_llm_response(
              query,
              model=model,
              provider=provider,
              npc=npc,
              stream=stream,
              messages=messages
          )

          result = response.get('response', '')
          if hasattr(result, '__iter__') and not isinstance(result, str):
              result = print_and_process_stream_with_markdown(
                  result, model, provider, show=True
              )
              st = globals().get('state')
              if st:
                  st._tool_output_streamed = True
          context['output'] = f"[Response delivered to user] {result}"
          context['messages'] = response.get('messages', messages)

          # Track usage
          if 'usage' in response and npc and hasattr(npc, 'shared_context'):
              usage = response['usage']
              npc.shared_context['session_input_tokens'] += usage.get('input_tokens', 0)
              npc.shared_context['session_output_tokens'] += usage.get('output_tokens', 0)
              npc.shared_context['turn_count'] += 1
