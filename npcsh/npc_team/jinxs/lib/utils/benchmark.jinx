jinx_name: benchmark
description: Run the npcsh benchmark suite against a model
inputs:
- model: ""
- provider: ""
- category: ""
- difficulty: ""
- task_id: ""
- timeout: "120"

steps:
  - name: run_benchmark
    engine: python
    code: |
      import sys

      model = ({{ model | default("") | tojson }}).strip()
      provider = ({{ provider | default("") | tojson }}).strip()
      category = ({{ category | default("") | tojson }}).strip() or None
      difficulty = ({{ difficulty | default("") | tojson }}).strip() or None
      task_id_filter = ({{ task_id | default("") | tojson }}).strip() or None
      timeout = int(({{ timeout | default("120") | tojson }}).strip() or "120")

      if not model:
          model = npc.model if npc and npc.model else state.chat_model if state else ""
      if not provider:
          provider = npc.provider if npc and npc.provider else state.chat_provider if state else ""

      if not model or not provider:
          context['output'] = "Error: model and provider are required. Usage: /benchmark model=qwen3:4b provider=ollama"
      else:
          try:
              from npcsh.benchmark.local_runner import run_benchmark

              report = run_benchmark(
                  model=model,
                  provider=provider,
                  category=category,
                  difficulty=difficulty,
                  task_id=task_id_filter,
                  timeout=timeout,
              )

              lines = []
              lines.append("## Benchmark Results: " + provider + "/" + model)
              lines.append("")
              lines.append("**Total:** " + str(report.total))
              lines.append("**Passed:** " + str(report.passed))
              lines.append("**Failed:** " + str(report.failed))
              lines.append("**Errors:** " + str(report.errors))
              pct = (100 * report.passed / report.total) if report.total > 0 else 0
              lines.append("**Pass Rate:** " + "{:.1f}%".format(pct))
              lines.append("**Duration:** " + "{:.1f}s".format(report.duration))
              lines.append("")

              if report.by_category:
                  lines.append("### By Category")
                  for cat, stats in sorted(report.by_category.items()):
                      p = stats.get("passed", 0)
                      t = stats.get("total", 0)
                      cat_pct = (100 * p / t) if t > 0 else 0
                      lines.append("- **{}:** {}/{} ({:.0f}%)".format(cat, p, t, cat_pct))
                  lines.append("")

              if report.by_difficulty:
                  lines.append("### By Difficulty")
                  for diff, stats in sorted(report.by_difficulty.items()):
                      p = stats.get("passed", 0)
                      t = stats.get("total", 0)
                      diff_pct = (100 * p / t) if t > 0 else 0
                      lines.append("- **{}:** {}/{} ({:.0f}%)".format(diff, p, t, diff_pct))
                  lines.append("")

              failed_tasks = [r for r in report.results if not r.passed]
              if failed_tasks:
                  lines.append("### Failed Tasks")
                  for r in failed_tasks[:20]:
                      err = r.error or "verification failed"
                      lines.append("- **{}** ({}): {}".format(r.task_id, r.category, err))

              context['output'] = "\n".join(lines)

          except Exception as e:
              import traceback
              context['output'] = "Error running benchmark: " + str(e) + "\n\n" + traceback.format_exc()
